
servers:
  - name: llm_server
    model: /path/to/your/model.gguf
    host: 0.0.0.0
    port: 50051
    threads: 40
    ctx-size: 2048
    batch-size: 512
    embedding: false
    api_key: your_api_key
  - name: embedding_server
    model: /path/to/your/model.gguf
    host: 0.0.0.0
    port: 50052
    threads: 40
    ctx-size: 2048
    batch-size: 512
    embedding: true
    api_key: your_api_key
  - name: llm_server-v2
    model: /path/to/your/model.gguf  # Model path
    host: 0.0.0.0  # Host address
    port: 50051  # Port number
    threads: 40  # Number of threads
    ctx_size: 2048  # Context size
    batch_size: 512  # Batch size
    embedding: false  # Enable embedding
    api_key: your_api_key  # API key
    verbose: true  # Verbose output
    seed: 1234  # RNG seed
    system-prompt-file: /path/to/your/system_prompt.txt  # System prompt file
    chat-template: /path/to/your/chat_template.txt  # Chat template file JINJA2
    prompt: "Your prompt here"  # Initial prompt
    temperature: 0.8  # Temperature
    top_k: 40  # Top-k sampling
    top_p: 0.9  # Top-p sampling
    repeat_penalty: 1.0  # Repeat penalty
    gpu_layers: 10  # Number of GPU layers


