
servers:
  - name: llm_server
    model: /path/to/your/model.gguf
    host: 0.0.0.0
    port: 50051
    threads: 40
    ctx-size: 2048
    batch-size: 512
    embedding: false
    api_key: your_api_key
  - name: embedding_server
    model: /path/to/your/model.gguf
    host: 0.0.0.0
    port: 50052
    threads: 40
    ctx-size: 2048
    batch-size: 512
    embedding: true
    api_key: your_api_key
  - name: llm_cli_v2
    model: /data/ai/lmstudio/Qwen/qwen2-72b.gguf
    threads: 40  # Number of threads
    ctx_size: 2048  # Context size
    batch_size: 512  # Batch size
    embedding: false  # Enable embedding
    # api_key: your_api_key  # API key
    verbose: true  # Verbose output
    seed: 1234  # RNG seed
    temp: "0.8"  # Temperature
    top-k: "40"  # Top-k sampling
    top-p: "0.9"  # Top-p sampling
    repeat_penalty: "1.5"  # Repeat penalty
    gpu-layers: "10"  # Number of GPU layers
    prompt: "In the context of LLMs, what is quantization"  # user prompt
  - name: llm_server_v2
    model: /data/ai/lmstudio/Qwen/qwen2-72b.gguf
    host: 0.0.0.0  # Host address
    port: 50054  # Port number
    threads: 40  # Number of threads
    ctx_size: 2048  # Context size
    batch_size: 512  # Batch size
    embedding: false  # Enable embedding
    # api_key: your_api_key  # API key
    verbose: true  # Verbose output
    seed: 1234  # RNG seed
    system-prompt-file: internal/prompts/system_prompt_readme.txt  # System prompt file
    # chat-template: /path/to/your/chat_template.txt  # Chat template file JINJA2
    temp: "0.8"  # Temperature
    top-k: "40"  # Top-k sampling
    top-p: "0.9"  # Top-p sampling
    repeat_penalty: "1.5"  # Repeat penalty
    gpu-layers: "30"  # Number of GPU layers
    mirostat: 1 # controlling perplexity during text generation.0 is disabled, 1 is Mirostat, and 2 is Mirostat 2.0.
    tensor-split: "0.3,0.7" # fraction of the model to offload to each GPU
    


